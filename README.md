- linear_regression.py - основная реализация линейной регрессии с поддержкой нескольких функций потерь и L2-регуляризацией
- descents.py - набор градиентных методов оптимизации + расписания learning rate
- main.ipynb - демонстрационный ноутбук с тестами, сравнением с sklearn, экспериентами на синтетических данных и реальном датасете autos.csv
- autos.csv - соответственно датасет, который использовался для тестов

Была реализована линейная регрессия с нуля на NumPy с поддержкой нескольких функций потерь L2-регуляризацией и методами градиентного спуска 

## Основные возможности:
- Функции потерь: MSE, MAE, Log-Cosh, Huber
- Регуляризация: L2 (Ridge)

- **Методы решения:** 
    1) Аналитическое решение через SVD
    2) Градиентный спуск и его вариации:
       - Vanilla Gradient Descent
       - Stochastic Gradient Descent
       - Stochastic Average Gradient
       - Momentum
       - Adam

- **Расписания learning rate:**
  - Constant
  - Time-Based decay (1 / (1 + t)^p)

- **Метрики:** R^2 score, история значений функций потерь

Сравение с sklearn.linear_model.LinearRegression
Эксперименты на синтетических и реальных данных

В main.ipynb проводится сравнение всех реализованных методов на датасете autos.csv (с предобработкой и масштабированием):
- Сходимость (графики loss)
- Влияние L2-регуляризации (μ) на качество и скорость
- Количество итераций до сходимости
- Train / Test MSE и R^2

Что можно улучшить и добавить?
- Поддержка fit_intercept (сейчас bias отсутствует)
- Кросс-валидация
- Grid/Random search по гиперпараметрам (lr, μ, batch_size)
- Early stopping по валидационной выборке
- Поддержка других loss-функций (Quantile loss, Poisson)
= Визуализация весов / feature importance
